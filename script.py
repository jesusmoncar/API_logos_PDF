import fitz  # PyMuPDF
import cv2
import numpy as np
from PIL import Image, ImageDraw
import io
import os
import json
from typing import List, Tuple, Dict, Optional
import logging
from pathlib import Path
import argparse
import pickle
import re
from scipy.spatial.distance import cosine
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# ML y Deep Learning
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision.models import resnet18
from sklearn.model_selection import train_test_split

# OCR
import easyocr

# Configurar logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Definir palabras clave para eliminar texto sensible
KEYWORDS_TO_REMOVE_MULTILANG = {
    'spanish': [
        "Ayuda y soporte", "¿Tienes alguna pregunta", "support.tiqets.com",
        "estrictamente personal", "logo", "marca", "empresa", "contacto", 
        "soporte técnico", "copyright", "derechos reservados", "marca registrada",
        "atención al cliente", "servicio al cliente", "política de privacidad",
        "términos y condiciones", "aviso legal", "cookies", "privacidad",
        "todos los derechos reservados", "prohibida su reproducción", "TIQETS INTERNATIONAL B.V.", "Generado por Tiqets"
    ],
    'english': [
        "Help and Support", "Do you have any questions", "support",
        "strictly personal", "logo", "brand", "company", "contact", 
        "technical support", "copyright", "all rights reserved", "trademark",
        "customer service", "customer support", "privacy policy", 
        "terms and conditions", "legal notice", "cookies", "privacy",
        "registered trademark", "reproduction prohibited", "help center",
        "contact us", "powered by", "generated by", "TIQETS INTERNATIONAL B.V."
    ],
    'french': [
        "Aide et support", "Avez-vous des questions", "support",
        "strictement personnel", "logo", "marque", "entreprise", "contact",
        "support technique", "droits d'auteur", "tous droits réservés",
        "service client", "politique de confidentialité", "conditions générales",
        "mention légale", "cookies", "confidentialité", "marque déposée", "TIQETS INTERNATIONAL B.V."
    ],
    'german': [
        "Hilfe und Support", "Haben Sie Fragen", "support", "streng persönlich",
        "logo", "marke", "unternehmen", "kontakt", "technischer support",
        "urheberrecht", "alle rechte vorbehalten", "kundendienst",
        "datenschutzrichtlinie", "geschäftsbedingungen", "impressum",
        "cookies", "datenschutz", "eingetragene marke", "TIQETS INTERNATIONAL B.V."
    ],
    'italian': [
        "Aiuto e supporto", "Hai domande", "support", "strettamente personale",
        "logo", "marca", "azienda", "contatto", "supporto tecnico",
        "copyright", "tutti i diritti riservati", "servizio clienti",
        "politica sulla privacy", "termini e condizioni", "note legali",
        "cookies", "privacy", "marchio registrato", "TIQETS INTERNATIONAL B.V."
    ],
    'portuguese': [
        "Ajuda e suporte", "Você tem alguma pergunta", "support", 
        "estritamente pessoal", "logo", "marca", "empresa", "contato",
        "suporte técnico", "direitos autorais", "todos os direitos reservados",
        "atendimento ao cliente", "política de privacidade", "termos e condições",
        "aviso legal", "cookies", "privacidade", "marca registrada", "TIQETS INTERNATIONAL B.V."
    ]
}

# Combinar todas las palabras clave en una sola lista
KEYWORDS_TO_REMOVE = []
for lang_keywords in KEYWORDS_TO_REMOVE_MULTILANG.values():
    KEYWORDS_TO_REMOVE.extend(lang_keywords)

#Blocks to remove
TEXT_BLOCKS_TO_REMOVE_MULTILANG = {
    'spanish': [
        "Ayuda y soporte", "¿Tienes alguna pregunta", "support.tiqets.com",
        "estrictamente personal", "contacto", "soporte técnico",
        "TIQETS INTERNATIONAL B.V.", "civitatis", 
        "(https://www.civitatis.com/es/privacidad/)", "Generado por Tiqets",
        "Política de privacidad", "Términos y condiciones", "Aviso legal",
        "Este documento es confidencial", "Uso interno únicamente", "TIQETS INTERNATIONAL B.V."
    ],
    'english': [
        "Help and Support", "Do you have any questions", "support.tiqets.com",
        "strictly personal", "contact", "technical support", "help center",
        "TIQETS INTERNATIONAL B.V.", "Generated by Tiqets", "Powered by",
        "Privacy Policy", "Terms and Conditions", "Legal Notice",
        "This document is confidential", "Internal use only", "Contact us",
        "Customer Service", "All rights reserved", "Copyright notice", "TIQETS INTERNATIONAL B.V."
    ],
    'french': [
        "Aide et support", "Avez-vous des questions", "support.tiqets.com",
        "strictement personnel", "contact", "support technique",
        "Politique de confidentialité", "Conditions générales", "Mentions légales",
        "Ce document est confidentiel", "Usage interne uniquement", "Nous contacter", "TIQETS INTERNATIONAL B.V."
    ],
    'german': [
        "Hilfe und Support", "Haben Sie Fragen", "support.tiqets.com",
        "streng persönlich", "kontakt", "technischer support",
        "Datenschutzrichtlinie", "Geschäftsbedingungen", "Impressum",
        "Dieses Dokument ist vertraulich", "Nur für internen Gebrauch", "TIQETS INTERNATIONAL B.V."
    ],
    'italian': [
        "Aiuto e supporto", "Hai domande", "support.tiqets.com",
        "strettamente personale", "contatto", "supporto tecnico",
        "Politica sulla privacy", "Termini e condizioni", "Note legali",
        "Questo documento è riservato", "Solo per uso interno", "TIQETS INTERNATIONAL B.V."
    ],
    'portuguese': [
        "Ajuda e suporte", "Você tem alguma pergunta", "support.tiqets.com",
        "estritamente pessoal", "contato", "suporte técnico",
        "Política de privacidade", "Termos e condições", "Aviso legal",
        "Este documento é confidencial", "Apenas para uso interno", "TIQETS INTERNATIONAL B.V."
    ]
}

# Patrones de URLs y enlaces para detectar
URL_PATTERNS = [
    r'https?://[^\s<>"{}|\\^`\[\]]+',  # URLs HTTP/HTTPS
    r'www\.[^\s<>"{}|\\^`\[\]]+',      # URLs que empiezan con www
    r'[a-zA-Z0-9.-]+\.(com|org|net|edu|gov|mil|int|co|uk|es|fr|de|it|ru|cn|jp|au|ca)\b[^\s]*',  # Dominios comunes
    r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',  # Emails
    r'tel:\+?[0-9\s\-\(\)]+',          # Enlaces telefónicos
    r'mailto:[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}',  # Enlaces mailto
]

# Dominios específicos a eliminar
DOMAINS_TO_REMOVE = [
    "tiqets.com", "support.tiqets.com", "civitatis.com", "viator.com", 
    "getyourguide.com", "tripadvisor.com", "booking.com", "expedia.com",
    "ticketmaster.com", "eventbrite.com", "stubhub.com"
]

def create_multilang_keywords(languages=None):
    """
    Crea listas de palabras clave personalizadas para idiomas específicos.
    """
    if languages is None:
        languages = ['spanish', 'english', 'french']  # Idiomas por defecto
    
    custom_keywords = []
    custom_blocks = []
    
    for lang in languages:
        if lang in KEYWORDS_TO_REMOVE_MULTILANG:
            custom_keywords.extend(KEYWORDS_TO_REMOVE_MULTILANG[lang])
        if lang in TEXT_BLOCKS_TO_REMOVE_MULTILANG:
            custom_blocks.extend(TEXT_BLOCKS_TO_REMOVE_MULTILANG[lang])
    
    # Agregar palabras específicas adicionales
    additional_keywords = [
        "copyright", "all rights reserved", "marca registrada", "tous droits réservés",
        "alle rechte vorbehalten", "tutti i diritti riservati", "todos os direitos reservados"
    ]
    custom_keywords.extend(additional_keywords)
    custom_blocks.extend(additional_keywords)
    
    return custom_keywords, custom_blocks
    
class LogoDataset(Dataset):
    """Dataset personalizado para entrenar el detector de logos."""
    
    def __init__(self, image_paths: List[str], labels: List[int], transform=None):
        self.image_paths = image_paths
        self.labels = labels
        self.transform = transform
        
    def __len__(self):
        return len(self.image_paths)
    
    def __getitem__(self, idx):
        try:
            image_path = self.image_paths[idx]
            image = Image.open(image_path).convert('RGB')
            label = self.labels[idx]
            
            if self.transform:
                image = self.transform(image)
                
            return image, label
        except Exception as e:
            logger.error(f"Error cargando imagen {self.image_paths[idx]}: {e}")
            # Retornar una imagen en blanco como fallback
            blank_image = torch.zeros(3, 224, 224)
            return blank_image, self.labels[idx]

class LogoDetectorCNN(nn.Module):
    """Red neuronal convolucional para detectar logos."""
    
    def __init__(self, num_classes: int = 2):
        super(LogoDetectorCNN, self).__init__()
        self.backbone = resnet18(pretrained=True)
        # Congelar las primeras capas para transfer learning
        for param in list(self.backbone.parameters())[:-10]:
            param.requires_grad = False
        # Modificar la última capa
        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)
        
    def forward(self, x):
        return self.backbone(x)

class OCRDetector:
    """Detector de texto OCR multiidioma mejorado."""
    
    def __init__(self, languages=None):
        if languages is None:
            # Idiomas por defecto: español, inglés, francés
            languages = ['es', 'en', 'fr']
        
        try:
            self.reader = easyocr.Reader(languages, gpu=False)
            logger.info(f"OCR EasyOCR inicializado con idiomas: {', '.join(languages)}")
        except Exception as e:
            logger.error(f"Error inicializando OCR multiidioma: {e}")
            self.reader = None
    
    def detect_text_in_logo(self, image: np.ndarray, logo_keywords: List[str] = None) -> Dict:
        """
        Detecta texto en una imagen y determina si contiene palabras clave de logos en múltiples idiomas.
        """
        if logo_keywords is None:
            logo_keywords = KEYWORDS_TO_REMOVE
        
        result = {
            'has_text': False,
            'is_logo_text': False,
            'text_content': '',
            'confidence': 0.0,
            'text_boxes': [],
            'detected_language': 'unknown'
        }
        
        if self.reader is None:
            return result
        
        try:
            # Asegurar que la imagen esté en el formato correcto
            if len(image.shape) == 3 and image.shape[2] > 3:
                image = image[:, :, :3]  # Solo RGB
            
            detections = self.reader.readtext(image)
            
            all_text = []
            for detection in detections:
                if len(detection) >= 3:
                    bbox, text, confidence = detection[0], detection[1], detection[2]
                    result['text_content'] += text + ' '
                    all_text.append(text.lower())
                    result['text_boxes'].append({
                        'bbox': bbox,
                        'text': text,
                        'confidence': confidence
                    })
                    
                    if confidence > result['confidence']:
                        result['confidence'] = confidence
            
            result['has_text'] = len(result['text_content'].strip()) > 0
            
            # Detectar idioma probable basándose en palabras clave encontradas
            if result['has_text']:
                result['detected_language'] = self.detect_language(all_text)
            
            # Verificar si contiene palabras clave de logos en cualquier idioma
            text_lower = result['text_content'].lower()
            result['is_logo_text'] = any(keyword.lower() in text_lower for keyword in logo_keywords)
            
            if result['has_text']:
                logger.debug(f"Texto detectado ({result['detected_language']}): '{result['text_content'][:100]}...', es_logo: {result['is_logo_text']}")
                
        except Exception as e:
            logger.error(f"Error en OCR multiidioma: {e}")
            
        return result
    
    def detect_language(self, text_list: List[str]) -> str:
        """
        Detecta el idioma probable basándose en las palabras encontradas.
        """
        # Palabras indicadoras por idioma
        language_indicators = {
            'spanish': ['ayuda', 'soporte', 'empresa', 'contacto', 'política', 'términos', 'aviso', 'derechos', "Generado por Tiqets"],
            'english': ['help', 'support', 'company', 'contact', 'policy', 'terms', 'notice', 'rights', 'powered'],
            'french': ['aide', 'support', 'entreprise', 'contact', 'politique', 'conditions', 'mention', 'droits'],
            'german': ['hilfe', 'support', 'unternehmen', 'kontakt', 'richtlinie', 'bedingungen', 'impressum'],
            'italian': ['aiuto', 'supporto', 'azienda', 'contatto', 'politica', 'condizioni', 'note'],
            'portuguese': ['ajuda', 'suporte', 'empresa', 'contato', 'política', 'termos', 'aviso', 'direitos']
        }
        
        language_scores = {lang: 0 for lang in language_indicators.keys()}
        
        for text in text_list:
            text_lower = text.lower()
            for lang, indicators in language_indicators.items():
                for indicator in indicators:
                    if indicator in text_lower:
                        language_scores[lang] += 1
        
        # Retornar el idioma con mayor puntuación
        if max(language_scores.values()) > 0:
            return max(language_scores, key=language_scores.get)
        
        return 'unknown'

class LinkDetector:
    """Detector y eliminador de enlaces en PDFs."""
    
    def __init__(self):
        # Compilar patrones regex para eficiencia
        self.url_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in URL_PATTERNS]
        self.domains_to_remove = DOMAINS_TO_REMOVE
        
    def detect_links_in_text(self, text: str) -> List[Dict]:
        """
        Detecta enlaces en un texto dado.
        """
        links_found = []
        
        for pattern in self.url_patterns:
            matches = pattern.finditer(text)
            for match in matches:
                link_info = {
                    'text': match.group(),
                    'start': match.start(),
                    'end': match.end(),
                    'type': self._classify_link_type(match.group())
                }
                links_found.append(link_info)
        
        # Verificar dominios específicos
        for domain in self.domains_to_remove:
            if domain.lower() in text.lower():
                # Buscar contexto del dominio
                domain_pattern = re.compile(rf'\S*{re.escape(domain)}\S*', re.IGNORECASE)
                matches = domain_pattern.finditer(text)
                for match in matches:
                    link_info = {
                        'text': match.group(),
                        'start': match.start(),
                        'end': match.end(),
                        'type': 'domain_specific'
                    }
                    links_found.append(link_info)
        
        return links_found
    
    def _classify_link_type(self, link_text: str) -> str:
        """Clasifica el tipo de enlace."""
        link_lower = link_text.lower()
        
        if link_lower.startswith(('http://', 'https://')):
            return 'url_http'
        elif link_lower.startswith('www.'):
            return 'url_www'
        elif link_lower.startswith('mailto:'):
            return 'email_mailto'
        elif link_lower.startswith('tel:'):
            return 'phone_tel'
        elif '@' in link_text and '.' in link_text:
            return 'email'
        elif any(tld in link_lower for tld in ['.com', '.org', '.net', '.edu']):
            return 'domain'
        else:
            return 'unknown'
    
    def remove_link_annotations(self, page) -> int:
        """
        Elimina anotaciones de enlaces (clickeables) de una página PDF.
        """
        removed_count = 0
        
        try:
            # Obtener todas las anotaciones de la página
            annotations = page.annots()
            
            for annot in annotations:
                try:
                    annot_dict = annot.info
                    annot_type = annot_dict.get('content', '')
                    
                    # Verificar si es una anotación de enlace
                    if (annot.type[1] == 'Link' or 
                        'uri' in annot_dict or 
                        any(domain in str(annot_dict).lower() for domain in self.domains_to_remove)):
                        
                        # Eliminar la anotación
                        page.delete_annot(annot)
                        removed_count += 1
                        logger.debug(f"Eliminada anotación de enlace: {annot_dict}")
                        
                except Exception as e:
                    logger.warning(f"Error procesando anotación: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error eliminando anotaciones de enlaces: {e}")
        
        return removed_count
    
    def remove_links_from_text_blocks(self, page, aggressive: bool = False) -> int:
        """
        Elimina texto que contiene enlaces de los bloques de texto de la página.
        """
        removed_count = 0
        
        try:
            # Método 1: Buscar bloques de texto con enlaces
            blocks = page.get_text("blocks")
            
            for block in blocks:
                if len(block) >= 5:
                    x0, y0, x1, y1, text = block[:5]
                    
                    # Detectar enlaces en el texto
                    links_found = self.detect_links_in_text(text)
                    
                    if links_found:
                        # Si aggressive=True, elimina todo el bloque
                        if aggressive:
                            rect = fitz.Rect(x0, y0, x1, y1)
                            page.draw_rect(rect, color=(1, 1, 1), fill=(1, 1, 1), width=0)
                            removed_count += 1
                            logger.info(f"LINK Eliminado bloque con enlaces: '{text[:50]}...'")
                        else:
                            # Eliminar solo las partes con enlaces
                            for link in links_found:
                                # Buscar la posición específica del enlace
                                link_instances = page.search_for(link['text'])
                                for inst in link_instances:
                                    expanded_rect = fitz.Rect(
                                        inst.x0 - 2, inst.y0 - 1,
                                        inst.x1 + 2, inst.y1 + 1
                                    )
                                    page.draw_rect(expanded_rect, color=(1, 1, 1), fill=(1, 1, 1), width=0)
                                    removed_count += 1
                                    logger.info(f"LINK Eliminado enlace: '{link['text']}'")
            
            # Método 2: Buscar enlaces específicos por patrón
            for pattern in self.url_patterns:
                # Buscar texto que coincida con patrones de URL
                text_dict = page.get_text("dict")
                for block in text_dict.get("blocks", []):
                    if "lines" in block:
                        for line in block["lines"]:
                            for span in line.get("spans", []):
                                span_text = span.get("text", "")
                                if pattern.search(span_text):
                                    # Crear rectángulo para el span
                                    bbox = span.get("bbox")
                                    if bbox:
                                        rect = fitz.Rect(bbox)
                                        expanded_rect = fitz.Rect(
                                            rect.x0 - 2, rect.y0 - 1,
                                            rect.x1 + 2, rect.y1 + 1
                                        )
                                        page.draw_rect(expanded_rect, color=(1, 1, 1), fill=(1, 1, 1), width=0)
                                        removed_count += 1
                                        logger.info(f"LINK Eliminado texto con patrón URL: '{span_text[:30]}...'")
                                        
        except Exception as e:
            logger.error(f"Error eliminando enlaces de texto: {e}")
        
        return removed_count

class TrainingDataAnalyzer:
    """Analizador que extrae características específicas de los datos de entrenamiento."""
    
    def __init__(self, training_dir: str = "training_data"):
        self.training_dir = Path(training_dir)
        self.logo_features = []
        self.no_logo_features = []
        self.feature_scaler = StandardScaler()
        self.logo_histograms = []
        self.no_logo_histograms = []
        self.is_initialized = False
        
    def extract_image_features(self, image: np.ndarray) -> np.ndarray:
        """Extrae un vector de características de una imagen."""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image
            
            h, w = gray.shape
            if h == 0 or w == 0:
                return np.zeros(20)
            
            # Redimensionar para consistencia
            gray = cv2.resize(gray, (64, 64))
            
            features = []
            
            # 1. Estadísticas básicas
            features.extend([
                np.mean(gray),
                np.std(gray),
                np.min(gray),
                np.max(gray),
                np.median(gray)
            ])
            
            # 2. Análisis de bordes
            edges = cv2.Canny(gray, 50, 150)
            edge_density = np.sum(edges > 0) / (64 * 64)
            features.append(edge_density)
            
            # 3. Análisis de textura (LBP simplificado)
            lbp_hist = self.compute_lbp_histogram(gray)
            features.extend(lbp_hist[:5])  # Solo primeros 5 bins
            
            # 4. Análisis de gradientes
            grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
            grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
            grad_mag = np.sqrt(grad_x**2 + grad_y**2)
            features.extend([
                np.mean(grad_mag),
                np.std(grad_mag)
            ])
            
            # 5. Análisis de simetría
            symmetry_h = self.compute_symmetry(gray, axis=1)
            symmetry_v = self.compute_symmetry(gray, axis=0)
            features.extend([symmetry_h, symmetry_v])
            
            # 6. Análisis de densidad de píxeles
            non_white = np.sum(gray < 240) / (64 * 64)
            very_dark = np.sum(gray < 50) / (64 * 64)
            features.extend([non_white, very_dark])
            
            # 7. Ratio de aspecto normalizado
            aspect_ratio = min(h, w) / max(h, w) if max(h, w) > 0 else 0
            features.append(aspect_ratio)
            
            return np.array(features[:20])  # Limitamos a 20 características
            
        except Exception as e:
            logger.error(f"Error extrayendo características: {e}")
            return np.zeros(20)
    
    def compute_lbp_histogram(self, image: np.ndarray, num_points: int = 8, radius: int = 1) -> np.ndarray:
        """Computa un histograma LBP simplificado."""
        h, w = image.shape
        lbp = np.zeros_like(image)
        
        for i in range(radius, h - radius):
            for j in range(radius, w - radius):
                center = image[i, j]
                code = 0
                for k in range(num_points):
                    angle = 2 * np.pi * k / num_points
                    x = int(round(i + radius * np.cos(angle)))
                    y = int(round(j + radius * np.sin(angle)))
                    if 0 <= x < h and 0 <= y < w:
                        if image[x, y] >= center:
                            code |= (1 << k)
                lbp[i, j] = code
        
        hist, _ = np.histogram(lbp.ravel(), bins=16, range=(0, 256))
        hist = hist.astype(float)
        hist /= (hist.sum() + 1e-7)
        return hist
    
    def compute_symmetry(self, image: np.ndarray, axis: int) -> float:
        """Computa la simetría de una imagen."""
        try:
            if axis == 0:  # Simetría vertical
                h = image.shape[0]
                top = image[:h//2, :]
                bottom = np.flipud(image[h//2:, :])
                min_h = min(top.shape[0], bottom.shape[0])
                if min_h == 0:
                    return 0.0
                diff = np.mean(np.abs(top[:min_h, :].astype(float) - bottom[:min_h, :].astype(float)))
            else:  # Simetría horizontal
                w = image.shape[1]
                left = image[:, :w//2]
                right = np.fliplr(image[:, w//2:])
                min_w = min(left.shape[1], right.shape[1])
                if min_w == 0:
                    return 0.0
                diff = np.mean(np.abs(left[:, :min_w].astype(float) - right[:, :min_w].astype(float)))
            
            # Normalizar la diferencia
            symmetry = max(0, 1 - diff / 255.0)
            return symmetry
        except:
            return 0.0
    
    def compute_histogram_similarity(self, image: np.ndarray, reference_histograms: List[np.ndarray]) -> float:
        """Computa similitud basada en histogramas."""
        try:
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image
            
            gray = cv2.resize(gray, (64, 64))
            hist = cv2.calcHist([gray], [0], None, [32], [0, 256])
            hist = hist.flatten()
            hist = hist / (np.sum(hist) + 1e-7)
            
            max_similarity = 0.0
            for ref_hist in reference_histograms:
                # Usar correlación de histogramas
                similarity = cv2.compareHist(hist.astype(np.float32), ref_hist.astype(np.float32), cv2.HISTCMP_CORREL)
                max_similarity = max(max_similarity, similarity)
            
            return max(0.0, max_similarity)
        except:
            return 0.0
    
    def initialize_from_training_data(self) -> bool:
        """Inicializa el analizador con los datos de entrenamiento."""
        if not self.training_dir.exists():
            logger.warning(f"Directorio de entrenamiento no encontrado: {self.training_dir}")
            return False
        
        logger.info("Analizando datos de entrenamiento...")
        
        # Procesar logos
        logo_dir = self.training_dir / "logos"
        if logo_dir.exists():
            for ext in ["*.png", "*.jpg", "*.jpeg"]:
                for img_path in logo_dir.glob(ext):
                    try:
                        img = cv2.imread(str(img_path))
                        if img is not None:
                            features = self.extract_image_features(img)
                            self.logo_features.append(features)
                            
                            # Guardar histograma
                            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                            gray = cv2.resize(gray, (64, 64))
                            hist = cv2.calcHist([gray], [0], None, [32], [0, 256])
                            hist = hist.flatten()
                            hist = hist / (np.sum(hist) + 1e-7)
                            self.logo_histograms.append(hist)
                            
                            logger.debug(f"Logo procesado: {img_path.name}")
                    except Exception as e:
                        logger.warning(f"Error procesando logo {img_path}: {e}")
        
        # Procesar no-logos
        no_logo_dir = self.training_dir / "no_logos"
        if no_logo_dir.exists():
            for ext in ["*.png", "*.jpg", "*.jpeg"]:
                for img_path in no_logo_dir.glob(ext):
                    try:
                        img = cv2.imread(str(img_path))
                        if img is not None:
                            features = self.extract_image_features(img)
                            self.no_logo_features.append(features)
                            
                            # Guardar histograma
                            gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                            gray = cv2.resize(gray, (64, 64))
                            hist = cv2.calcHist([gray], [0], None, [32], [0, 256])
                            hist = hist.flatten()
                            hist = hist / (np.sum(hist) + 1e-7)
                            self.no_logo_histograms.append(hist)
                            
                            logger.debug(f"No-logo procesado: {img_path.name}")
                    except Exception as e:
                        logger.warning(f"Error procesando no-logo {img_path}: {e}")
        
        if len(self.logo_features) == 0 or len(self.no_logo_features) == 0:
            logger.error("No se encontraron suficientes ejemplos de entrenamiento")
            return False
        
        # Normalizar características
        all_features = np.vstack([self.logo_features, self.no_logo_features])
        self.feature_scaler.fit(all_features)
        
        self.logo_features = [self.feature_scaler.transform(f.reshape(1, -1)).flatten() for f in self.logo_features]
        self.no_logo_features = [self.feature_scaler.transform(f.reshape(1, -1)).flatten() for f in self.no_logo_features]
        
        self.is_initialized = True
        logger.info(f"Analizador inicializado: {len(self.logo_features)} logos, {len(self.no_logo_features)} no-logos")
        return True
    
    def classify_image(self, image: np.ndarray) -> Tuple[bool, float]:
        """Clasifica una imagen basándose en similitud con los datos de entrenamiento."""
        if not self.is_initialized:
            return False, 0.0
        
        try:
            # Extraer características
            features = self.extract_image_features(image)
            features = self.feature_scaler.transform(features.reshape(1, -1)).flatten()
            
            # Calcular similitud con logos
            logo_similarities = []
            for logo_feat in self.logo_features:
                similarity = 1 - cosine(features, logo_feat)
                logo_similarities.append(max(0, similarity))
            
            # Calcular similitud con no-logos
            no_logo_similarities = []
            for no_logo_feat in self.no_logo_features:
                similarity = 1 - cosine(features, no_logo_feat)
                no_logo_similarities.append(max(0, similarity))
            
            # Similitud por histogramas
            logo_hist_sim = self.compute_histogram_similarity(image, self.logo_histograms)
            no_logo_hist_sim = self.compute_histogram_similarity(image, self.no_logo_histograms)
            
            # Combinar similitudes
            max_logo_sim = max(logo_similarities) if logo_similarities else 0
            max_no_logo_sim = max(no_logo_similarities) if no_logo_similarities else 0
            
            # Ponderación: 70% características, 30% histogramas
            logo_score = 0.7 * max_logo_sim + 0.3 * logo_hist_sim
            no_logo_score = 0.7 * max_no_logo_sim + 0.3 * no_logo_hist_sim
            
            # Decisión conservadora: debe ser significativamente más similar a logos
            is_logo = logo_score > no_logo_score and logo_score > 0.6
            confidence = logo_score - no_logo_score + 0.5  # Normalizar a [0,1]
            confidence = max(0, min(1, confidence))
            
            logger.debug(f"Clasificación por similitud: logo={logo_score:.3f}, no_logo={no_logo_score:.3f}, "
                        f"final={'LOGO' if is_logo else 'NO-LOGO'} ({confidence:.3f})")
            
            return is_logo, confidence
            
        except Exception as e:
            logger.error(f"Error en clasificación por similitud: {e}")
            return False, 0.0

class InpaintingProcessor:
    """Procesador mejorado para rellenar áreas de logos."""
    
    def inpaint_logo_area(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """
        Rellena el área del logo usando técnicas de inpainting mejoradas.
        """
        try:
            # Convertir imagen a uint8 si es necesario
            if image.dtype != np.uint8:
                image = (image * 255).astype(np.uint8) if image.max() <= 1.0 else image.astype(np.uint8)
            
            # Asegurar que la máscara sea binaria
            if mask.dtype != np.uint8:
                mask = (mask * 255).astype(np.uint8) if mask.max() <= 1.0 else mask.astype(np.uint8)
            
            # Método Telea (más rápido y efectivo para logos)
            inpainted = cv2.inpaint(image, mask, inpaintRadius=5, flags=cv2.INPAINT_TELEA)
            
            # Si falla, usar método alternativo
            if inpainted is None or np.array_equal(inpainted, image):
                inpainted = self.simple_inpaint(image, mask)
            
            return inpainted
            
        except Exception as e:
            logger.error(f"Error en inpainting: {e}")
            return self.simple_inpaint(image, mask)
    
    def simple_inpaint(self, image: np.ndarray, mask: np.ndarray) -> np.ndarray:
        """Método simple de inpainting usando interpolación."""
        result = image.copy()
        
        # Expandir máscara para obtener contexto
        kernel = np.ones((7, 7), np.uint8)
        expanded_mask = cv2.dilate(mask, kernel, iterations=1)
        border_mask = expanded_mask - mask
        
        # Rellenar con color promedio del borde
        if len(image.shape) == 3:
            for channel in range(image.shape[2]):
                border_pixels = image[:, :, channel][border_mask > 0]
                if len(border_pixels) > 0:
                    avg_color = np.mean(border_pixels)
                    result[:, :, channel][mask > 0] = avg_color
        else:
            border_pixels = image[border_mask > 0]
            if len(border_pixels) > 0:
                avg_color = np.mean(border_pixels)
                result[mask > 0] = avg_color
        
        return result

class AdvancedPDFLogoRemover:
    """Sistema mejorado de detección y eliminación de logos."""
    
    def __init__(self, model_path: str = "logo_classifier.pth", training_data_dir: str = "training_data"):
        self.model_path = model_path
        self.model = None
        
        # Inicializar componentes
        self.ocr_detector = OCRDetector()
        self.inpainter = InpaintingProcessor()
        self.training_analyzer = TrainingDataAnalyzer(training_data_dir)
        self.link_detector = LinkDetector()
        
        # Inicializar analizador de datos de entrenamiento
        logger.info("Inicializando analizador de datos de entrenamiento...")
        self.training_analyzer.initialize_from_training_data()
        
        # Transformaciones para el modelo
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        # Cargar modelo si existe
        self.load_model()
    
    def heuristic_logo_detection(self, image: np.ndarray) -> Tuple[bool, float]:
        """
        Detección mejorada y más conservadora de logos usando características heurísticas.
        """
        try:
            height, width = image.shape[:2]
            
            # Filtro de tamaño más estricto para logos típicos
            min_size = min(height, width)
            max_size = max(height, width)
            
            # Los logos suelen ser medianos/pequeños, no muy grandes ni muy pequeños
            if min_size < 30 or max_size > 400 or min_size > 200:
                return False, 0.0
            
            # Convertir a escala de grises
            if len(image.shape) == 3:
                gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
            else:
                gray = image
            
            # 1. Análisis de contornos estructurados (logos tienen formas definidas)
            edges = cv2.Canny(gray, 50, 150)
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # Evaluar calidad de contornos
            significant_contours = [c for c in contours if cv2.contourArea(c) > (height * width * 0.05)]
            contour_score = min(len(significant_contours) / 3.0, 1.0) if len(significant_contours) > 0 else 0.0
            
            # 2. Análisis de texto/símbolos (logos suelen tener texto o formas geométricas)
            # Detectar líneas horizontales y verticales (características de logos)
            kernel_h = np.ones((1, max(width//10, 3)), np.uint8)
            kernel_v = np.ones((max(height//10, 3), 1), np.uint8)
            
            lines_h = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel_h)
            lines_v = cv2.morphologyEx(edges, cv2.MORPH_OPEN, kernel_v)
            
            line_ratio = (np.sum(lines_h > 0) + np.sum(lines_v > 0)) / (height * width)
            line_score = min(line_ratio * 5, 1.0)
            
            # 3. Análisis de densidad y distribución
            # Los logos tienen distribución no uniforme de píxeles
            non_white_pixels = np.sum(gray < 240) / (height * width)
            density_score = min(non_white_pixels * 2, 1.0) if non_white_pixels > 0.1 else 0.0
            
            # 4. Análisis de aspectos geométricos
            aspect_ratio = max_size / min_size
            # Los logos suelen ser más cuadrados o rectangulares moderados
            aspect_score = max(0, 1 - abs(aspect_ratio - 1.5) / 3) if aspect_ratio <= 4 else 0.0
            
            # 5. Análisis de varianza local (logos tienen regiones con diferentes intensidades)
            # Dividir imagen en bloques y analizar varianza
            block_size = min(height//3, width//3, 20)
            if block_size > 5:
                variances = []
                for i in range(0, height-block_size, block_size):
                    for j in range(0, width-block_size, block_size):
                        block = gray[i:i+block_size, j:j+block_size]
                        variances.append(np.var(block))
                
                if variances:
                    avg_var = np.mean(variances)
                    var_score = min(avg_var / 1000, 1.0) if avg_var > 50 else 0.0
                else:
                    var_score = 0.0
            else:
                var_score = 0.0
            
            # Combinación más conservadora con pesos ajustados
            confidence = (
                contour_score * 0.25 +    # Contornos definidos
                line_score * 0.20 +       # Líneas/estructura
                density_score * 0.20 +    # Densidad apropiada
                aspect_score * 0.15 +     # Aspecto adecuado
                var_score * 0.20          # Varianza local
            )
            
            # Umbral más alto para ser más conservador
            is_logo = confidence > 0.5
            
            logger.debug(f"Análisis heurístico mejorado: contornos={contour_score:.3f}, "
                        f"líneas={line_score:.3f}, densidad={density_score:.3f}, "
                        f"aspecto={aspect_score:.3f}, varianza={var_score:.3f}, final={confidence:.3f}")
            
            return is_logo, confidence
            
        except Exception as e:
            logger.error(f"Error en detección heurística: {e}")
            return False, 0.0
    
    def extract_and_analyze_images(self, page) -> List[Dict]:
        """
        Extrae y analiza imágenes de una página de forma más robusta.
        """
        images = []
        
        try:
            image_list = page.get_images(full=True)
            logger.debug(f"Encontradas {len(image_list)} imágenes en la página")
            
            for img_index, img in enumerate(image_list):
                try:
                    xref = img[0]
                    
                    # Extraer imagen usando método más robusto
                    try:
                        base_image = page.parent.extract_image(xref)
                        image_bytes = base_image["image"]
                        image_ext = base_image["ext"]
                        
                        if not image_bytes:
                            logger.warning(f"Imagen {img_index} sin datos")
                            continue
                            
                    except Exception as e:
                        logger.warning(f"No se pudo extraer imagen {img_index}: {e}")
                        continue
                    
                    # Convertir a array numpy
                    try:
                        pil_image = Image.open(io.BytesIO(image_bytes))
                        if pil_image.mode not in ['RGB', 'RGBA']:
                            pil_image = pil_image.convert('RGB')
                        image_array = np.array(pil_image)
                        
                        if image_array.size == 0:
                            continue
                            
                    except Exception as e:
                        logger.warning(f"Error convirtiendo imagen {img_index}: {e}")
                        continue
                    
                    # Obtener rectángulo de la imagen
                    try:
                        image_rects = page.get_image_rects(xref)
                        if not image_rects:
                            logger.warning(f"No se encontró rectángulo para imagen {img_index}")
                            continue
                        rect = image_rects[0]
                    except Exception as e:
                        logger.warning(f"Error obteniendo rectángulo de imagen {img_index}: {e}")
                        continue
                    
                    # Análisis ML si disponible
                    if self.model is not None:
                        try:
                            is_logo_ml, confidence_ml = self.predict_logo(image_array)
                        except Exception as e:
                            logger.warning(f"Error en predicción ML para imagen {img_index}: {e}")
                            is_logo_ml, confidence_ml = False, 0.0
                    else:
                        is_logo_ml, confidence_ml = False, 0.0
                    
                    # Análisis heurístico
                    try:
                        is_logo_heur, confidence_heur = self.heuristic_logo_detection(image_array)
                    except Exception as e:
                        logger.warning(f"Error en análisis heurístico para imagen {img_index}: {e}")
                        is_logo_heur, confidence_heur = False, 0.0
                    
                    # Análisis por similitud con datos de entrenamiento
                    try:
                        is_logo_similarity, confidence_similarity = self.training_analyzer.classify_image(image_array)
                    except Exception as e:
                        logger.warning(f"Error en análisis por similitud para imagen {img_index}: {e}")
                        is_logo_similarity, confidence_similarity = False, 0.0
                    
                    # Análisis OCR
                    try:
                        ocr_result = self.ocr_detector.detect_text_in_logo(image_array)
                    except Exception as e:
                        logger.warning(f"Error en OCR para imagen {img_index}: {e}")
                        ocr_result = {'has_text': False, 'is_logo_text': False, 'text_content': '', 'confidence': 0.0}
                    
                    # Combinación de resultados con prioridad en datos de entrenamiento
                    if self.training_analyzer.is_initialized:
                        if self.model is not None:
                            # Con modelo ML y datos de entrenamiento
                            ml_weight = 0.3
                            heur_weight = 0.15
                            similarity_weight = 0.45  # Mayor peso a similitud
                            ocr_weight = 0.1
                        else:
                            # Solo con datos de entrenamiento (sin modelo ML)
                            ml_weight = 0.0
                            heur_weight = 0.25
                            similarity_weight = 0.65  # Máximo peso a similitud
                            ocr_weight = 0.1
                    else:
                        # Sin datos de entrenamiento (modo legacy)
                        if self.model is not None:
                            ml_weight = 0.6
                            heur_weight = 0.25
                            similarity_weight = 0.0
                            ocr_weight = 0.15
                        else:
                            ml_weight = 0.0
                            heur_weight = 0.7
                            similarity_weight = 0.0
                            ocr_weight = 0.3
                    
                    # OCR: solo considerar como logo si realmente tiene texto típico de logos
                    text_content = ocr_result.get('text_content', '').lower().strip()
                    logo_indicators = ['logo', 'brand', 'company', 'corp', 'inc', 'ltd', 'copyright', '©', '®', '™']
                    has_logo_text = any(indicator in text_content for indicator in logo_indicators)
                    
                    # Ajustar score OCR basado en contenido real
                    if has_logo_text:
                        ocr_score = 0.9
                    elif ocr_result.get('has_text', False) and len(text_content) > 3:
                        # Texto genérico podría ser logo, pero menos probable
                        ocr_score = 0.3
                    else:
                        ocr_score = 0.1
                    
                    combined_score = (confidence_ml * ml_weight + 
                                    confidence_heur * heur_weight + 
                                    confidence_similarity * similarity_weight +
                                    ocr_score * ocr_weight)
                    
                    # Decisión final más rigurosa basada en datos de entrenamiento
                    if self.training_analyzer.is_initialized:
                        # Si el analizador de similitud dice que NO es logo, respetarlo
                        if confidence_similarity < 0.3:
                            is_logo_final = False
                        # Si dice que SÍ es logo con alta confianza, también respetarlo
                        elif is_logo_similarity and confidence_similarity > 0.7:
                            is_logo_final = True
                        # En casos intermedios, usar score combinado con umbral alto
                        else:
                            is_logo_final = combined_score > 0.75
                    else:
                        # Sin datos de entrenamiento, usar umbral conservador
                        is_logo_final = combined_score > 0.6
                    
                    analysis = {
                        'xref': xref,
                        'index': img_index,
                        'image': image_array,
                        'rect': rect,
                        'size': (image_array.shape[1], image_array.shape[0]),
                        'ml_prediction': {
                            'is_logo': is_logo_ml,
                            'confidence': confidence_ml
                        },
                        'heuristic_prediction': {
                            'is_logo': is_logo_heur,
                            'confidence': confidence_heur
                        },
                        'similarity_prediction': {
                            'is_logo': is_logo_similarity,
                            'confidence': confidence_similarity
                        },
                        'ocr_analysis': ocr_result,
                        'final_prediction': {
                            'is_logo': is_logo_final,
                            'combined_score': combined_score
                        }
                    }
                    
                    images.append(analysis)
                    
                    logger.info(f"IMAGE Imagen {img_index} ({image_array.shape[1]}x{image_array.shape[0]}): "
                              f"ML={confidence_ml:.3f}, Heur={confidence_heur:.3f}, "
                              f"Simil={confidence_similarity:.3f}, "
                              f"OCR={'Sí' if has_logo_text else 'Parcial' if ocr_result.get('has_text', False) else 'No'}, "
                              f"Final={combined_score:.3f} {'OK LOGO' if is_logo_final else 'X NO-LOGO'}")
                    
                except Exception as e:
                    logger.error(f"Error procesando imagen {img_index}: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error extrayendo imágenes de la página: {e}")
            
        return images
    
    def remove_logo_advanced(self, page, logo_data: Dict) -> bool:
        """
        Elimina un logo usando el método más efectivo disponible.
        """
        try:
            rect = logo_data['rect']
            
            # Método 1: Rectángulo blanco simple (más confiable)
            page.draw_rect(rect, color=(1, 1, 1), fill=(1, 1, 1), width=0)
            
            # Método 2: Intentar inpainting si la imagen es suficientemente grande
            image = logo_data.get('image')
            if image is not None and image.size > 1000:  # Solo para imágenes grandes
                try:
                    height, width = image.shape[:2]
                    mask = np.zeros((height, width), dtype=np.uint8)
                    mask.fill(255)  # Máscara completa
                    
                    inpainted = self.inpainter.inpaint_logo_area(image, mask)
                    # Nota: PyMuPDF no permite reemplazar imágenes fácilmente,
                    # por lo que mantenemos el método del rectángulo
                except Exception as e:
                    logger.debug(f"Inpainting falló, usando rectángulo: {e}")
            
            logger.info(f"OK Logo eliminado en rectángulo: {rect}")
            return True
            
        except Exception as e:
            logger.error(f"ERROR Error eliminando logo: {e}")
            return False
    
    def prepare_training_data(self, data_dir: str) -> Tuple[List[str], List[int]]:
        """Prepara datos de entrenamiento con validación de calidad."""
        data_path = Path(data_dir)
        
        if not data_path.exists():
            raise FileNotFoundError(f"El directorio {data_dir} no existe")
        
        image_paths = []
        labels = []
        invalid_images = []
        
        # Logos (etiqueta 1)
        logo_dir = data_path / "logos"
        if logo_dir.exists():
            logger.info(f"Procesando imágenes de logos desde {logo_dir}")
            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif', '*.svg']:
                for img_path in logo_dir.glob(ext):
                    try:
                        # Validar que la imagen se puede cargar
                        test_img = Image.open(img_path)
                        if test_img.size[0] > 10 and test_img.size[1] > 10:  # Mínimo 10x10
                            image_paths.append(str(img_path))
                            labels.append(1)
                            logger.debug(f"Logo añadido: {img_path.name} ({test_img.size})")
                        else:
                            invalid_images.append(str(img_path))
                        test_img.close()
                    except Exception as e:
                        logger.warning(f"Error cargando {img_path}: {e}")
                        invalid_images.append(str(img_path))
        
        # No logos (etiqueta 0)
        no_logo_dir = data_path / "no_logos"
        if no_logo_dir.exists():
            logger.info(f"Procesando imágenes sin logos desde {no_logo_dir}")
            for ext in ['*.jpg', '*.jpeg', '*.png', '*.bmp', '*.gif']:
                for img_path in no_logo_dir.glob(ext):
                    try:
                        # Validar que la imagen se puede cargar
                        test_img = Image.open(img_path)
                        if test_img.size[0] > 10 and test_img.size[1] > 10:  # Mínimo 10x10
                            image_paths.append(str(img_path))
                            labels.append(0)
                            logger.debug(f"No-logo añadido: {img_path.name} ({test_img.size})")
                        else:
                            invalid_images.append(str(img_path))
                        test_img.close()
                    except Exception as e:
                        logger.warning(f"Error cargando {img_path}: {e}")
                        invalid_images.append(str(img_path))
        
        if invalid_images:
            logger.warning(f"Se encontraron {len(invalid_images)} imágenes inválidas que serán ignoradas")
        
        # Verificar balance de datos
        logo_count = labels.count(1)
        no_logo_count = labels.count(0)
        
        if logo_count == 0 or no_logo_count == 0:
            raise ValueError("Se necesitan imágenes tanto de logos como de no-logos para entrenar")
        
        logger.info(f"Datos preparados: {len(image_paths)} imágenes válidas "
                   f"({logo_count} logos, {no_logo_count} no logos)")
        
        # Reportar balance
        if abs(logo_count - no_logo_count) > max(logo_count, no_logo_count) * 0.5:
            logger.warning(f"Datos desbalanceados: {logo_count} logos vs {no_logo_count} no-logos. "
                         f"Considera agregar más imágenes a la categoría menor.")
        
        return image_paths, labels
    
    def train_model(self, data_dir: str, epochs: int = 10, batch_size: int = 16) -> None:
        """Entrena el modelo de detección."""
        logger.info("INFO Iniciando entrenamiento del modelo...")
        
        try:
            image_paths, labels = self.prepare_training_data(data_dir)
            
            if len(image_paths) < 10:
                raise ValueError("Se necesitan al menos 10 imágenes para entrenar")
            
            # Dividir datos
            train_paths, val_paths, train_labels, val_labels = train_test_split(
                image_paths, labels, test_size=0.2, random_state=42, stratify=labels
            )
            
            # Crear datasets
            train_dataset = LogoDataset(train_paths, train_labels, self.transform)
            val_dataset = LogoDataset(val_paths, val_labels, self.transform)
            
            # Data loaders (pin_memory=False para evitar warning cuando no hay GPU)
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            pin_memory = torch.cuda.is_available()
            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=pin_memory)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0, pin_memory=pin_memory)
            
            # Modelo
            self.model = LogoDetectorCNN(num_classes=2)
            self.model.to(device)
            logger.info(f"Usando dispositivo: {device}")
            
            # Optimizador y loss
            criterion = nn.CrossEntropyLoss()
            optimizer = optim.Adam(self.model.parameters(), lr=0.001, weight_decay=1e-4)
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)
            
            best_val_acc = 0.0
            
            for epoch in range(epochs):
                # Entrenamiento
                self.model.train()
                train_loss = 0.0
                train_correct = 0
                train_total = 0
                
                for batch_idx, (images, labels) in enumerate(train_loader):
                    images, labels = images.to(device), labels.to(device)
                    
                    optimizer.zero_grad()
                    outputs = self.model(images)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()
                    
                    train_loss += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    train_total += labels.size(0)
                    train_correct += (predicted == labels).sum().item()
                
                # Validación
                self.model.eval()
                val_correct = 0
                val_total = 0
                
                with torch.no_grad():
                    for images, labels in val_loader:
                        images, labels = images.to(device), labels.to(device)
                        outputs = self.model(images)
                        _, predicted = torch.max(outputs.data, 1)
                        val_total += labels.size(0)
                        val_correct += (predicted == labels).sum().item()
                
                train_acc = 100 * train_correct / train_total
                val_acc = 100 * val_correct / val_total
                
                logger.info(f'Época {epoch+1}/{epochs}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%')
                
                # Guardar mejor modelo
                if val_acc > best_val_acc:
                    best_val_acc = val_acc
                    self.save_model()
                
                scheduler.step()
            
            logger.info(f"OK Entrenamiento completado. Mejor precisión: {best_val_acc:.2f}%")
            
        except Exception as e:
            logger.error(f"ERROR Error en entrenamiento: {e}")
            raise
    
    def save_model(self) -> None:
        """Guarda el modelo."""
        if self.model is not None:
            torch.save(self.model.state_dict(), self.model_path)
            logger.info(f"SAVE Modelo guardado en {self.model_path}")
    
    def load_model(self) -> bool:
        """Carga el modelo."""
        if os.path.exists(self.model_path):
            try:
                self.model = LogoDetectorCNN(num_classes=2)
                self.model.load_state_dict(torch.load(self.model_path, map_location='cpu'))
                self.model.eval()
                logger.info(f"OK Modelo cargado desde {self.model_path}")
                return True
            except Exception as e:
                logger.error(f"ERROR Error cargando modelo: {e}")
                return False
        return False
    
    def predict_logo(self, image: np.ndarray) -> Tuple[bool, float]:
        """Predice si una imagen es un logo."""
        if self.model is None:
            return False, 0.0
        
        try:
            # Preparar imagen
            if len(image.shape) == 3:
                pil_image = Image.fromarray(image)
            else:
                pil_image = Image.fromarray(image).convert('RGB')
            
            input_tensor = self.transform(pil_image).unsqueeze(0)
            
            # Predicción
            device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            self.model.to(device)
            input_tensor = input_tensor.to(device)
            
            with torch.no_grad():
                outputs = self.model(input_tensor)
                probabilities = torch.softmax(outputs, dim=1)
                confidence, predicted = torch.max(probabilities, 1)
                
                is_logo = predicted.item() == 1
                conf_score = confidence.item()
                
                return is_logo, conf_score
                
        except Exception as e:
            logger.error(f"Error en predicción ML: {e}")
            return False, 0.0
    
    def remove_text_blocks(self, doc, keywords: List[str] = None) -> int:
        """
        Elimina bloques de texto específicos del documento.
        """
        if keywords is None:
            keywords = TEXT_BLOCKS_TO_REMOVE
        
        removed_count = 0
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            
            try:
                # Método 1: Buscar por bloques de texto
                blocks = page.get_text("blocks")
                for block in blocks:
                    if len(block) >= 5:
                        x0, y0, x1, y1, text = block[:5]
                        text_lower = text.lower()
                        
                        if any(keyword.lower() in text_lower for keyword in keywords):
                            rect = fitz.Rect(x0, y0, x1, y1)
                            page.draw_rect(rect, color=(1, 1, 1), fill=(1, 1, 1), width=0)
                            removed_count += 1
                            logger.info(f"CLEAN Eliminado bloque en página {page_num + 1}: '{text[:30]}...'")
                
                # Método 2: Buscar por palabras específicas
                for keyword in keywords:
                    text_instances = page.search_for(keyword)
                    for inst in text_instances:
                        # Expandir el rectángulo un poco
                        expanded_rect = fitz.Rect(
                            inst.x0 - 5, inst.y0 - 2, 
                            inst.x1 + 5, inst.y1 + 2
                        )
                        page.draw_rect(expanded_rect, color=(1, 1, 1), fill=(1, 1, 1), width=0)
                        removed_count += 1
                        logger.info(f"CLEAN Eliminada palabra '{keyword}' en página {page_num + 1}")
                        
            except Exception as e:
                logger.error(f"Error eliminando texto en página {page_num + 1}: {e}")
        
        return removed_count
    
    def remove_links(self, doc, aggressive: bool = False) -> Dict:
        """
        Elimina enlaces del documento PDF.
        """
        stats = {
            'total_link_annotations_removed': 0,
            'total_link_text_removed': 0,
            'pages_processed': 0,
            'details': []
        }
        
        for page_num in range(len(doc)):
            page = doc[page_num]
            page_stats = {
                'page_number': page_num + 1,
                'link_annotations_removed': 0,
                'link_text_removed': 0
            }
            
            try:
                # Eliminar anotaciones de enlaces (clickeables)
                annotations_removed = self.link_detector.remove_link_annotations(page)
                page_stats['link_annotations_removed'] = annotations_removed
                stats['total_link_annotations_removed'] += annotations_removed
                
                # Eliminar texto con enlaces
                text_removed = self.link_detector.remove_links_from_text_blocks(page, aggressive)
                page_stats['link_text_removed'] = text_removed
                stats['total_link_text_removed'] += text_removed
                
                if annotations_removed > 0 or text_removed > 0:
                    logger.info(f"LINK Página {page_num + 1}: {annotations_removed} anotaciones, {text_removed} textos con enlaces eliminados")
                
                stats['details'].append(page_stats)
                stats['pages_processed'] += 1
                
            except Exception as e:
                logger.error(f"Error eliminando enlaces en página {page_num + 1}: {e}")
                continue
        
        total_removed = stats['total_link_annotations_removed'] + stats['total_link_text_removed']
        if total_removed > 0:
            logger.info(f"LINK Total enlaces eliminados: {stats['total_link_annotations_removed']} anotaciones + {stats['total_link_text_removed']} texto = {total_removed}")
        
        return stats
    
    def process_pdf(self, input_path: str, output_path: str, 
                   confidence_threshold: float = 0.5,
                   remove_text_blocks_flag: bool = True,
                   remove_links_flag: bool = True,
                   aggressive_link_removal: bool = False,
                   text_keywords: List[str] = None) -> Dict:
        """
        Procesa un PDF completo eliminando logos y texto específico.
        """
        import time
        
        # Iniciar medición de tiempo
        start_time = time.time()
        
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"El archivo {input_path} no existe")
        
        stats = {
            'total_pages': 0,
            'total_images': 0,
            'logos_detected': 0,
            'logos_removed': 0,
            'text_blocks_removed': 0,
            'links_removed': 0,
            'link_annotations_removed': 0,
            'link_text_removed': 0,
            'processing_details': [],
            'processing_time_seconds': 0.0
        }
        
        try:
            # Obtener solo el nombre del archivo para mostrar
            pdf_name = os.path.basename(input_path)
            
            logger.info(f"\n{'='*80}")
            logger.info(f"🔄 INICIANDO PROCESAMIENTO: {pdf_name}")
            logger.info(f"{'='*80}")
            logger.info(f"FILE Abriendo PDF: {input_path}")
            doc = fitz.open(input_path)
            stats['total_pages'] = len(doc)
            
            # Paso 1: Eliminar bloques de texto
            if remove_text_blocks_flag:
                logger.info("CLEAN Eliminando bloques de texto específicos...")
                keywords = text_keywords or TEXT_BLOCKS_TO_REMOVE
                text_removed = self.remove_text_blocks(doc, keywords)
                stats['text_blocks_removed'] = text_removed
            
            # Paso 2: Eliminar enlaces
            if remove_links_flag:
                logger.info("LINK Eliminando enlaces del documento...")
                link_stats = self.remove_links(doc, aggressive_link_removal)
                stats['link_annotations_removed'] = link_stats['total_link_annotations_removed']
                stats['link_text_removed'] = link_stats['total_link_text_removed']
                stats['links_removed'] = stats['link_annotations_removed'] + stats['link_text_removed']
            
            # Paso 3: Procesar imágenes y logos
            logger.info("SEARCH Analizando imágenes y logos...")
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                logger.info(f"PAGE Procesando página {page_num + 1}/{len(doc)}")
                
                analyzed_images = self.extract_and_analyze_images(page)
                stats['total_images'] += len(analyzed_images)
                
                page_details = {
                    'page_number': page_num + 1,
                    'images_analyzed': len(analyzed_images),
                    'logos_found': []
                }
                
                # Eliminar logos detectados con validación adicional
                for img_data in analyzed_images:
                    final_pred = img_data['final_prediction']
                    
                    if final_pred['is_logo']:
                        stats['logos_detected'] += 1
                    
                    # Validaciones con prioridad en similitud con datos de entrenamiento
                    should_remove = False
                    similarity_conf = img_data.get('similarity_prediction', {}).get('confidence', 0)
                    similarity_strong = similarity_conf > 0.7
                    
                    # REGLA 1: Si la similitud es muy alta (>0.8), eliminar directamente
                    if similarity_conf > 0.8:
                        should_remove = True
                        logger.info(f"OK Eliminación directa por alta similitud: {similarity_conf:.3f}")
                    
                    # REGLA 2: Si la similitud dice claramente que NO es logo (<0.3), no eliminar
                    elif similarity_conf < 0.3:
                        should_remove = False
                        logger.info(f"INFO No eliminar por baja similitud: {similarity_conf:.3f}")
                    
                    # REGLA 3: Para casos intermedios, aplicar validaciones tradicionales
                    elif final_pred['combined_score'] >= confidence_threshold:
                        # Validación de tamaño
                        img_size = img_data.get('size', (0, 0))
                        size_ok = 20 <= min(img_size) <= 800  # Rango más amplio
                        
                        # Validaciones de confianza
                        ml_strong = img_data['ml_prediction']['confidence'] > 0.6
                        heur_strong = img_data['heuristic_prediction']['confidence'] > 0.5
                        ocr_strong = 'logo' in img_data['ocr_analysis'].get('text_content', '').lower()
                        
                        has_strong_indicator = similarity_strong or ml_strong or heur_strong or ocr_strong
                        should_remove = size_ok and has_strong_indicator
                        
                        if should_remove:
                            img_size = img_data.get('size', (0, 0))
                            logger.info(f"OK Eliminando logo validado: score={final_pred['combined_score']:.3f}, "
                                      f"size={img_size}, Simil={similarity_conf:.3f}")
                        else:
                            img_size = img_data.get('size', (0, 0))
                            logger.info(f"WARN Logo detectado pero no validado: score={final_pred['combined_score']:.3f}, "
                                      f"size={img_size}, similitud={similarity_conf:.3f}")
                    
                    if should_remove:
                        success = self.remove_logo_advanced(page, img_data)
                        if success:
                            stats['logos_removed'] += 1
                            
                            page_details['logos_found'].append({
                                'ml_confidence': img_data['ml_prediction']['confidence'],
                                'heuristic_confidence': img_data['heuristic_prediction']['confidence'],
                                'similarity_confidence': img_data.get('similarity_prediction', {}).get('confidence', 0),
                                'has_logo_text': img_data['ocr_analysis'].get('is_logo_text', False),
                                'text_detected': img_data['ocr_analysis'].get('text_content', '')[:50],
                                'final_score': final_pred['combined_score'],
                                'rect': img_data['rect']
                            })
                
                stats['processing_details'].append(page_details)
            
            # Guardar PDF procesado
            logger.info(f"SAVE Guardando PDF procesado: {output_path}")
            doc.save(output_path)
            doc.close()
            
            logger.info(f"✅ COMPLETADO: {pdf_name}")
            logger.info(f"{'='*80}\n")
            
        except Exception as e:
            logger.error(f"ERROR Error procesando PDF: {e}")
            raise
        finally:
            # Calcular tiempo de procesamiento
            end_time = time.time()
            processing_time = end_time - start_time
            stats['processing_time_seconds'] = round(processing_time, 2)
            
            # Log del tiempo de procesamiento
            minutes = int(processing_time // 60)
            seconds = processing_time % 60
            if minutes > 0:
                logger.info(f"TIME Tiempo de procesamiento: {minutes}m {seconds:.2f}s ({processing_time:.2f}s total)")
            else:
                logger.info(f"TIME Tiempo de procesamiento: {processing_time:.2f}s")
        
        return stats
    
    def process_pdf_batch(self, input_dir: str = "input", output_dir: str = "outputModificado",
                         confidence_threshold: float = 0.5,
                         remove_text_blocks_flag: bool = True,
                         remove_links_flag: bool = True,
                         aggressive_link_removal: bool = False,
                         text_keywords: List[str] = None) -> Dict:
        """
        Procesa todos los PDFs de una carpeta y los guarda en otra carpeta.
        """
        import time
        
        # Iniciar medición de tiempo total
        batch_start_time = time.time()
        
        input_path = Path(input_dir)
        output_path = Path(output_dir)
        
        # Crear directorio de salida si no existe
        if not output_path.exists():
            output_path.mkdir(parents=True, exist_ok=True)
            logger.info(f"Directorio de salida creado: {output_path}")
        
        # Buscar todos los archivos PDF
        pdf_files = list(input_path.glob("*.pdf"))
        
        if not pdf_files:
            logger.warning(f"No se encontraron archivos PDF en {input_path}")
            return {
                'total_files': 0,
                'processed_files': 0,
                'failed_files': 0,
                'files_details': []
            }
        
        logger.info(f"\n{'#'*80}")
        logger.info(f"🚀 INICIANDO PROCESAMIENTO EN LOTE")
        logger.info(f"{'#'*80}")
        logger.info(f"📂 Directorio de entrada: {input_path}")
        logger.info(f"📂 Directorio de salida: {output_path}")
        logger.info(f"📄 Archivos encontrados: {len(pdf_files)} PDFs")
        logger.info(f"{'#'*80}\n")
        
        batch_stats = {
            'total_files': len(pdf_files),
            'processed_files': 0,
            'failed_files': 0,
            'files_details': [],
            'total_processing_time_seconds': 0.0,
            'average_time_per_file': 0.0
        }
        
        for i, pdf_file in enumerate(pdf_files, 1):
            try:
                # Crear ruta de salida con el mismo nombre
                output_file = output_path / pdf_file.name
                
                logger.info(f"\n{'='*80}")
                logger.info(f"🔄 PROCESANDO ARCHIVO {i}/{len(pdf_files)}: {pdf_file.name}")
                logger.info(f"{'='*80}")
                
                # Procesar el PDF individual
                file_stats = self.process_pdf(
                    str(pdf_file),
                    str(output_file),
                    confidence_threshold=confidence_threshold,
                    remove_text_blocks_flag=remove_text_blocks_flag,
                    remove_links_flag=remove_links_flag,
                    aggressive_link_removal=aggressive_link_removal,
                    text_keywords=text_keywords
                )
                
                file_stats['input_file'] = str(pdf_file)
                file_stats['output_file'] = str(output_file)
                file_stats['status'] = 'success'
                
                batch_stats['files_details'].append(file_stats)
                batch_stats['processed_files'] += 1
                
                logger.info(f"✅ ARCHIVO COMPLETADO: {pdf_file.name}")
                logger.info(f"{'='*80}\n")
                
            except Exception as e:
                logger.error(f"ERROR Error procesando {pdf_file.name}: {e}")
                
                file_stats = {
                    'input_file': str(pdf_file),
                    'output_file': str(output_path / pdf_file.name),
                    'status': 'failed',
                    'error': str(e),
                    'total_pages': 0,
                    'total_images': 0,
                    'logos_detected': 0,
                    'logos_removed': 0,
                    'text_blocks_removed': 0
                }
                
                batch_stats['files_details'].append(file_stats)
                batch_stats['failed_files'] += 1
        
        # Calcular tiempo total de procesamiento
        batch_end_time = time.time()
        total_processing_time = batch_end_time - batch_start_time
        batch_stats['total_processing_time_seconds'] = round(total_processing_time, 2)
        
        # Calcular tiempo promedio por archivo procesado
        if batch_stats['processed_files'] > 0:
            batch_stats['average_time_per_file'] = round(total_processing_time / batch_stats['processed_files'], 2)
        
        # Log del tiempo total de procesamiento
        minutes = int(total_processing_time // 60)
        seconds = total_processing_time % 60
        logger.info(f"\n{'#'*80}")
        logger.info(f"📊 RESUMEN FINAL DEL PROCESAMIENTO EN LOTE")
        logger.info(f"{'#'*80}")
        if minutes > 0:
            logger.info(f"⏱️  Tiempo total: {minutes}m {seconds:.2f}s ({total_processing_time:.2f}s)")
        else:
            logger.info(f"⏱️  Tiempo total: {total_processing_time:.2f}s")
        logger.info(f"📁 Archivos procesados: {batch_stats['processed_files']}/{batch_stats['total_files']}")
        if batch_stats['failed_files'] > 0:
            logger.info(f"❌ Archivos fallidos: {batch_stats['failed_files']}")
        if batch_stats['processed_files'] > 0:
            logger.info(f"📈 Tiempo promedio por archivo: {batch_stats['average_time_per_file']:.2f}s")
        logger.info(f"{'#'*80}\n")
        
        return batch_stats
    
    def extract_candidates(self, input_path: str, output_dir: str) -> Dict:
        """
        Extrae todas las imágenes de un PDF y las clasifica como candidatos.
        """
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"El archivo {input_path} no existe")
        
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        stats = {
            'total_pages': 0,
            'total_images': 0,
            'logo_candidates': 0,
            'no_logo_candidates': 0,
            'extracted_files': []
        }
        
        logger.info(f"Extrayendo candidatos de: {input_path}")
        logger.info(f"Carpeta de salida: {output_dir}")
        
        try:
            doc = fitz.open(input_path)
            stats['total_pages'] = len(doc)
            
            for page_num in range(len(doc)):
                page = doc[page_num]
                logger.info(f"Procesando página {page_num + 1}/{len(doc)}")
                
                analyzed_images = self.extract_and_analyze_images(page)
                stats['total_images'] += len(analyzed_images)
                
                for img_index, img_data in enumerate(analyzed_images):
                    try:
                        # Usar nuestro sistema de clasificación avanzado
                        final_pred = img_data['final_prediction']
                        similarity_conf = img_data.get('similarity_prediction', {}).get('confidence', 0)
                        
                        # Clasificación mejorada
                        if similarity_conf > 0.7:
                            label = "logo_candidate"
                            confidence = similarity_conf
                            stats['logo_candidates'] += 1
                        elif final_pred['is_logo'] and final_pred['combined_score'] > 0.6:
                            label = "logo_candidate"
                            confidence = final_pred['combined_score']
                            stats['logo_candidates'] += 1
                        else:
                            label = "no_logo_candidate"
                            confidence = 1 - final_pred['combined_score']
                            stats['no_logo_candidates'] += 1
                        
                        # Crear nombre de archivo informativo
                        filename = f"page{page_num+1}_img{img_index}_{label}_{confidence:.2f}.png"
                        save_path = output_path / filename
                        
                        # Guardar imagen
                        image_array = img_data['image']
                        if len(image_array.shape) == 3:
                            pil_image = Image.fromarray(image_array)
                        else:
                            pil_image = Image.fromarray(image_array).convert('RGB')
                        
                        pil_image.save(str(save_path))
                        
                        # Guardar información del archivo
                        file_info = {
                            'filename': filename,
                            'page': page_num + 1,
                            'image_index': img_index,
                            'classification': label,
                            'confidence': confidence,
                            'size': img_data['size'],
                            'ml_confidence': img_data['ml_prediction']['confidence'],
                            'similarity_confidence': similarity_conf,
                            'has_text': img_data['ocr_analysis'].get('has_text', False),
                            'text_content': img_data['ocr_analysis'].get('text_content', '')[:50]
                        }
                        
                        stats['extracted_files'].append(file_info)
                        
                        logger.debug(f"Extraido: {filename} - {label} ({confidence:.3f})")
                        
                    except Exception as e:
                        logger.error(f"Error guardando imagen {img_index} de página {page_num + 1}: {e}")
                        continue
            
            doc.close()
            
            # Crear reporte JSON
            report_path = output_path / "extraction_report.json"
            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, indent=2, ensure_ascii=False)
            
            logger.info(f"Extracción completada: {stats['total_images']} imágenes")
            logger.info(f"Candidatos a logo: {stats['logo_candidates']}")
            logger.info(f"No logos: {stats['no_logo_candidates']}")
            logger.info(f"Reporte guardado: {report_path}")
            
        except Exception as e:
            logger.error(f"Error extrayendo candidatos: {e}")
            raise
        
        return stats

def main():
    """Función principal mejorada."""
    
    parser = argparse.ArgumentParser(description="Sistema avanzado de eliminación de logos y texto")
    parser.add_argument("--input", "-i", default="input.pdf", help="Archivo PDF de entrada (ignorado si se usa --batch)")
    parser.add_argument("--output", "-o", default="output_clean.pdf", help="Archivo PDF de salida (ignorado si se usa --batch)")
    parser.add_argument("--batch", "-b", action="store_true", help="Procesar todos los PDFs de la carpeta 'pdf'")
    parser.add_argument("--input_dir", default="input", help="Carpeta de entrada para procesamiento en lote")
    parser.add_argument("--output_dir", default="outputModificado", help="Carpeta de salida para procesamiento en lote")
    parser.add_argument("--train_data", "-t", default="training_data", help="Directorio con datos de entrenamiento")
    parser.add_argument("--confidence", "-c", type=float, default=0.5, help="Umbral de confianza (0.0-1.0)")
    parser.add_argument("--no_text", action="store_true", help="No eliminar bloques de texto")
    parser.add_argument("--train", action="store_true", help="Entrenar modelo antes de procesar")
    parser.add_argument("--epochs", type=int, default=10, help="Épocas de entrenamiento")
    parser.add_argument("--extract-candidates", action="store_true", help="Extraer imágenes candidatas en lugar de procesar PDF")
    parser.add_argument("--candidates_dir", default="candidates", help="Carpeta donde guardar candidatos extraídos")
    parser.add_argument("--remove-links", action="store_true", help="Eliminar enlaces y URLs del PDF")
    parser.add_argument("--aggressive-links", action="store_true", help="Eliminación agresiva de enlaces (incluye texto con URLs)")
    
    args = parser.parse_args()

    
    
    print("=" * 60)
    if getattr(args, 'extract_candidates', False):
        print("EXTRACCION DE CANDIDATOS - CLASIFICACION DE IMAGENES")
    elif args.batch:
        print("PROCESAMIENTO EN LOTE - ELIMINACION DE LOGOS Y TEXTO")
    else:
        print("SISTEMA AVANZADO DE ELIMINACION DE LOGOS Y TEXTO")
    print("=" * 60)
    
    # Verificar modo de procesamiento
    if getattr(args, 'extract_candidates', False):
        # Modo extracción de candidatos
        if not os.path.exists(args.input):
            print(f"ERROR: No se encontró el archivo {args.input}")
            print("TIP Tip: Usa --input para especificar la ruta del PDF")
            return
        
        print(f"FILE Archivo de entrada: {args.input}")
        print(f"FOLDER Carpeta de candidatos: {args.candidates_dir}")
    elif args.batch:
        # Verificar carpeta de entrada
        if not os.path.exists(args.input_dir):
            print(f"ERROR: No se encontró la carpeta {args.input_dir}")
            print("TIP Tip: Crea una carpeta 'pdf' con los archivos PDF a procesar")
            return
        
        print(f"FOLDER Carpeta de entrada: {args.input_dir}")
        print(f"FOLDER Carpeta de salida: {args.output_dir}")
        print(f"INFO Umbral de confianza: {args.confidence}")
    else:
        # Verificar archivo de entrada
        if not os.path.exists(args.input):
            print(f"ERROR: No se encontró el archivo {args.input}")
            print("TIP Tip: Usa --input para especificar la ruta correcta o --batch para procesar en lote")
            return
        
        print(f"FILE Archivo de entrada: {args.input}")
        print(f"FILE Archivo de salida: {args.output}")
        print(f"INFO Umbral de confianza: {args.confidence}")
    
    # Crear instancia del sistema
    remover = AdvancedPDFLogoRemover()
    
    # Entrenar modelo si se solicita
    if args.train and os.path.exists(args.train_data):
        print(f"\nTRAIN Entrenando modelo con datos de {args.train_data}...")
        try:
            remover.train_model(args.train_data, epochs=args.epochs)
            print("OK Modelo entrenado exitosamente")
        except Exception as e:
            print(f"WARN Error en entrenamiento: {e}")
            print("INFO Continuando sin modelo ML...")
    elif args.train:
        print(f"WARN No se encontró directorio de entrenamiento: {args.train_data}")
    
    # Palabras clave personalizadas
    target_languages = ['spanish', 'english', 'french', 'german', 'italian', 'portuguese']
    custom_keywords, custom_text_blocks = create_multilang_keywords(target_languages)
    
    # Procesar PDF(s) o extraer candidatos
    try:
        if getattr(args, 'extract_candidates', False):
            # Extracción de candidatos
            print(f"\nEXTRACT Extrayendo candidatos...")
            stats = remover.extract_candidates(
                input_path=args.input,
                output_dir=args.candidates_dir
            )
        else:
            # Ajustar umbral basándose en si hay modelo ML
            effective_threshold = args.confidence
            if remover.model is None:
                logger.info("Sin modelo ML, usando umbral conservador")
                effective_threshold = max(args.confidence, 0.6)
            
            if args.batch:
                # Procesamiento en lote
                print(f"\nPROCESS Procesando PDFs en lote...")
                stats = remover.process_pdf_batch(
                    input_dir=args.input_dir,
                    output_dir=args.output_dir,
                    confidence_threshold=effective_threshold,
                    remove_text_blocks_flag=not args.no_text,
                    remove_links_flag=args.remove_links,
                    aggressive_link_removal=args.aggressive_links,
                    text_keywords=custom_keywords
                )
            else:
                # Procesamiento individual
                print(f"\nPROCESS Procesando PDF...")
                stats = remover.process_pdf(
                    args.input,
                    args.output,
                    confidence_threshold=effective_threshold,
                    remove_text_blocks_flag=not args.no_text,
                    remove_links_flag=args.remove_links,
                    aggressive_link_removal=args.aggressive_links,
                    text_keywords=custom_keywords
                )
        
        # Mostrar resultados
        print("\n" + "=" * 50)
        if getattr(args, 'extract_candidates', False):
            print("STATS RESULTADOS DE LA EXTRACCION")
        else:
            print("STATS RESULTADOS DEL PROCESAMIENTO")
        print("=" * 50)
        
        if getattr(args, 'extract_candidates', False):
            # Resultados de extracción de candidatos
            print(f"PAGE Páginas procesadas: {stats['total_pages']}")
            print(f"IMAGE Imágenes extraídas: {stats['total_images']}")
            print(f"LOGO Candidatos a logo: {stats['logo_candidates']}")
            print(f"NO-LOGO Candidatos descartados: {stats['no_logo_candidates']}")
            print(f"SAVE Imágenes guardadas en: {args.candidates_dir}")
            
            # Mostrar algunos ejemplos de candidatos a logo
            logo_files = [f for f in stats['extracted_files'] if 'logo_candidate' in f['classification']]
            if logo_files:
                print(f"\nDETAILS CANDIDATOS A LOGO ENCONTRADOS:")
                for i, file_info in enumerate(logo_files[:5]):  # Mostrar solo los primeros 5
                    print(f"  CANDIDATE {i+1}: {file_info['filename']}")
                    print(f"    - Confianza: {file_info['confidence']:.3f}")
                    print(f"    - Similitud: {file_info['similarity_confidence']:.3f}")
                    print(f"    - Tamaño: {file_info['size']}")
                    if file_info['has_text']:
                        print(f"    - Texto: '{file_info['text_content']}...'")
                if len(logo_files) > 5:
                    print(f"  ... y {len(logo_files) - 5} candidatos más")
        elif args.batch:
            # Resultados de procesamiento en lote
            print(f"FILES Archivos encontrados: {stats['total_files']}")
            print(f"OK Archivos procesados exitosamente: {stats['processed_files']}")
            print(f"ERROR Archivos con errores: {stats['failed_files']}")
            
            # Estadísticas totales
            total_pages = sum(f.get('total_pages', 0) for f in stats['files_details'])
            total_text_removed = sum(f.get('text_blocks_removed', 0) for f in stats['files_details'])
            total_images = sum(f.get('total_images', 0) for f in stats['files_details'])
            total_logos_detected = sum(f.get('logos_detected', 0) for f in stats['files_details'])
            total_logos_removed = sum(f.get('logos_removed', 0) for f in stats['files_details'])
            total_links_removed = sum(f.get('links_removed', 0) for f in stats['files_details'])
            
            print(f"PAGE Total páginas procesadas: {total_pages}")
            print(f"CLEAN Total bloques de texto eliminados: {total_text_removed}")
            print(f"IMAGE Total imágenes analizadas: {total_images}")
            print(f"INFO Total logos detectados: {total_logos_detected}")
            print(f"REMOVE Total logos eliminados: {total_logos_removed}")
            if args.remove_links:
                print(f"LINK Total enlaces eliminados: {total_links_removed}")
            print(f"SAVE Archivos guardados en: {args.output_dir}")
            
            # Mostrar detalles por archivo
            successful_files = [f for f in stats['files_details'] if f['status'] == 'success']
            files_with_logos = [f for f in successful_files if f.get('logos_removed', 0) > 0]
            
            if files_with_logos:
                print(f"\nDETAILS ARCHIVOS CON LOGOS ELIMINADOS:")
                for file_detail in files_with_logos:
                    print(f"FILE {Path(file_detail['input_file']).name}: {file_detail['logos_removed']} logos eliminados")
            
            # Mostrar archivos con errores
            failed_files = [f for f in stats['files_details'] if f['status'] == 'failed']
            if failed_files:
                print(f"\nERRORS ARCHIVOS CON ERRORES:")
                for file_detail in failed_files:
                    print(f"ERROR {Path(file_detail['input_file']).name}: {file_detail['error']}")
        else:
            # Resultados de procesamiento individual
            print(f"PAGE Páginas procesadas: {stats['total_pages']}")
            print(f"CLEAN Bloques de texto eliminados: {stats['text_blocks_removed']}")
            print(f"IMAGE Imágenes analizadas: {stats['total_images']}")
            print(f"INFO Logos detectados: {stats['logos_detected']}")
            print(f"REMOVE Logos eliminados: {stats['logos_removed']}")
            if args.remove_links:
                print(f"LINK Enlaces eliminados: {stats.get('links_removed', 0)}")
            print(f"SAVE Archivo generado: {args.output}")
            
            # Detalles por página (modo individual)
            if any(page_detail['logos_found'] for page_detail in stats['processing_details']):
                print(f"\nDETAILS DETALLES POR PÁGINA:")
                for page_detail in stats['processing_details']:
                    if page_detail['logos_found']:
                        print(f"\nPAGE Página {page_detail['page_number']}:")
                        for i, logo in enumerate(page_detail['logos_found']):
                            print(f"  LOGO Logo {i+1}:")
                            print(f"    - Confianza ML: {logo['ml_confidence']:.3f}")
                            print(f"    - Confianza heurística: {logo['heuristic_confidence']:.3f}")
                            print(f"    - Similitud con ejemplos: {logo.get('similarity_confidence', 0):.3f}")
                            print(f"    - Contiene texto: {'Sí' if logo['has_logo_text'] else 'No'}")
                            if logo['text_detected']:
                                print(f"    - Texto: '{logo['text_detected']}...'")
                            print(f"    - Puntuación final: {logo['final_score']:.3f}")
        
        if getattr(args, 'extract_candidates', False):
            print(f"\nOK ¡Extracción completada exitosamente!")
            print(f"INFO Las imágenes candidatas se guardaron en: {args.candidates_dir}/")
            print(f"INFO Reporte detallado: {args.candidates_dir}/extraction_report.json")
        else:
            print(f"\nOK ¡Procesamiento completado exitosamente!")
            if args.batch:
                print(f"INFO Los archivos procesados se guardaron en: {args.output_dir}/")
            else:
                print(f"INFO El archivo limpio se guardó como: {args.output}")
        
    except Exception as e:
        print(f"\nERROR Error durante el procesamiento: {e}")
        if args.batch:
            print("TIP Revisa que la carpeta 'pdf' exista y contenga archivos PDF válidos")
        else:
            print("TIP Revisa que el archivo PDF no esté corrupto o protegido")

def simple_demo():
    """Demostración simple para casos básicos."""
    print("DEMO MODO DEMOSTRACIÓN SIMPLE")
    
    # Archivos de ejemplo
    input_files = ["input.pdf", "document.pdf", "test.pdf", "tiqets.pdf"]
    input_file = None
    
    for file in input_files:
        if os.path.exists(file):
            input_file = file
            break
    
    if input_file is None:
        print("ERROR No se encontró ningún archivo PDF para procesar")
        print("TIP Coloca un archivo PDF llamado 'input.pdf' en este directorio")
        return
    
    output_file = f"clean_{input_file}"
    
    print(f"FILE Procesando: {input_file}")
    print(f"FILE Resultado: {output_file}")
    
    try:
        remover = AdvancedPDFLogoRemover()
        stats = remover.process_pdf(
            input_file,
            output_file,
            confidence_threshold=0.3,  # Más permisivo
            remove_text_blocks_flag=True
        )
        
        print(f"OK ¡Listo! Se eliminaron:")
        print(f"   - {stats['text_blocks_removed']} bloques de texto")
        print(f"   - {stats['logos_removed']} logos de {stats['logos_detected']} detectados")
        print(f"SAVE Archivo guardado: {output_file}")
        
    except Exception as e:
        print(f"ERROR Error: {e}")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) == 1:
        # Si no hay argumentos, usar modo simple
        simple_demo()
    else:
        # Si hay argumentos, usar modo avanzado
        main()